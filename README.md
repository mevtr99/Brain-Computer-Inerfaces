## plan
### I - Définition de la technologie
- **1. Caractéristiques des types de BCI**
- **2. Vaste et riche Histoire des BCIs bien avant Neuralink**
### II - Principes techniques
- **1. Cerveau, moelle épinière, nerfs**
- **2. Fonctionnement global BCIs**
### III - Articles les plus intéressants / principales avancées
- **1. Langage reconstruction**
- **2. Image reconstruction**
- **3. Mouvement**
### IV - Le cerveau, infinie complexité
### V - La course à l'alumette
- **1. Meta & BCIs**
- **2. Apple & BCIs**
- **3. Google & BCIs**
- **4. Bill Gates, Jeff Bezos & Synchron**
- **5. Neuralink**



# I - Définition de la technologie
Appartient à la Neural engineering / Neuro-ingénieurie = 1- discipline qui combine l'ingénierie et les neurosciences.  2- Repose sur l'idée que nous pouvons comprendre, réparer, ou améliorer notre cerveau. 3- Vise à développer des neuro-technologies pour intéragir avec le système nerveux et le cerveau en appliquant des principes d'ingénierie. 
Exemples d'applications de la neuro-ingénieurie : Neuroprothèses, Neuro-modulation, Imagerie cérébrale, et Interfaces Cerveau-Ordinateur (= ICO ou Interfaces cerveau-machine = ICM ou Brain-computer interface = BCI ou Interface neuronale direct = IND)

### 1. Caractéristiques des types des BCI

Plus ou moins invasives
|Méthode  d'acquisition des signaux | BCI non invasives | BCI invasives | BCI semi-invasives|
|------|----------|-------|-------|
| Définition | capteurs placés sur le cuir chevelu / près de la tête, qualité de signal inférieur, moins couteux |électrodes implantées directement dans le cerveau, offre des signaux + précis mais nécessite intervention chirurgicale, peut potentiellement capter signaux d'un groupe de neurones précis ou un neurone individuel | électrodes sont placées sous le crâne mais à l'extérieur de la matière grise, présente moins de risque que BCI invasives|
| Méthode d'acquisition du signal | EEG, MEG, IRM, IRMf, NIRS | microélectrodes intracorticales |ECoG, endovascular, dispositif SpiralE|
| Exemples |  | Telepathy de Neuralink, BrainGate, MoveAgain de Blackrock Neurotech||

Différentes applications
| Médical | Divertissement | Recherche|
|----------|-------|-------|
|communication des patients paralysés ; contrôle de prothèses motorisées par la pensée des ICM utilisées avec les prothèses permettent un contrôle neuronal direct pour des mouvements plus fluides et naturels ; restaurer des fonctions motrices et cognitives après une lésion cérébrale comme des implants cochléaires ou rétiniens pour restaurer l'audition ou la vision ; patients atteins de paralysie ou handicaps moteurs pour pouvoir interagie avec des ordis par la pensée, déplacer le curseur de la souris etc ; stimulation cérébrale profonde (DBS) pour traiter des maladies comme Parkinson ou des troubles psychiatriques; ...  |jeux vidéos contrôlés par la pensée, applications de réalité virtuelle|Etude du fonctionnement cérébral, Modélisation des processus de réfléxion humaine|

Deux directions de communication
| BCI unidirectionnelle |BCI bidirectionnelle|
|----------|-------|
|du cerveau vers une machine pour la contrôler par exemple ou de la machine au cerveau comme les implants cochléaires rétiniens pour restaurer l'audition| permettent à la fois d'émettre et de recevoir des signaux | 


### 2. Vaste et riche Histoire des BCIs bien avant Neuralink
Quelques éléments de l'histoire des BCI

<div align="center">
  <img src="https://github.com/user-attachments/assets/01406959-5740-4c2c-b39f-925c99e31139" alt="midjourneyv2">
  <p>image générée avec Midjourney </p>
</div>

**1791**
A commencé par des scientifiques qui se demandaient la diférence entre ce qui n'est pas vivant et ce qui est vivant. Ils se sont dit qu'il doit surement y avoir un esprit vital, une force vitale. Notamment un physicien : Luigi Galvani. Un jour Luigi disséquait des grenouilles et il avait genre un petit appareil qui pouvait créer un choc statique. Accidentellement, un petit choc a énervé les cuisses de grenouilles qui ont bougé. Et il s'est dit wow qu'est ce qu'il se passe. Pendant 5 années il a fait des experimentations pour comprendre qu'est ce que c'est. Il a publié un article et a dit Ok j'ai trouvé la force vitale. Ca sappelle ["Animal Electricity"](http://www.ampere.cnrs.fr/histoire/parcours-historique/galvani-volta/galvani/eng).
Un de ses potes était Axander Volta (on le connait des voltes), et il était super intéressé par cette Animal Electricity. Il a dit je pense pas ce soit de l'Animal Electricty, je pense y a une erreur dans ton travail. C'est le métal. Le métal est la chose qui énerve ces cuisses de grenouilles. Bataille pdt 10 années entre Volta et Galvani. Volta a commencé a faire ses expérimentations. Il a commencé a mixer les métaux et les empiler. Et c'est devenu la pile voltaique. Ou ce qu'on appelle maintenant la batterie electrique. Par la suite [Giovanni Aldini](https://numerabilis.u-paris.fr/partenaires/chn/docpdf/parent_aldini.pdf) le neuveu de Luigi Galvani, a continué les travaux et est aussi devenu connu pour avoir essayé de ranimer des cadavres avec de l'électricité. Et les gens allaient voir ces spectacles macabres mdrrrr. Bref

**1868-1963**
Vers le début des années 1900, l'EEG a commencé à être utilisé sur les animaux. En 1924 un homme du nom de Hans Berger a commencé à se demander si y avait pas un truc spécial avec l'univers, une manière de communiquer à travers l'espace et le temps. Et il a commencé à être obssédé par la telepathie, et à scanner le cerveau des gens. Il a réussi à record les premières ondes alphas. Premier Human EEG. En 1963, Denise Albe-Fessard pionnière d'une grande partie du travail sur les micro-électrodes qu'on utilise aujourd'hui. Elle a aussi réussi à identifier les régions du cerveau associées à des maladies de trouble du mouvement comme Parkinson. En 1963, Natalia Bekthereva surement la première à appliquer des méthodes de deep brain stimulation pour soigner les troubles du mouvement comme Parkinson. Toujours en 1963, José Delgado a créé un recepteur émetteur radio, a pratiqué une intervention chirurgicale sur un taurreau et a réussi à stimuler en sans fil le thalamus et autres parties du cerveau du taurreau pour l'arrêter en pleine attaque pour les corridas : [la vidéo](https://www.youtube.com/watch?v=WMySKxkwQqk&rco=1).

**1970-1990**
En 1973,  Jacques Vidal met la lumière sur ce domaine de recherche en publiant un article intitulé « Vers des communications cerveau-ordinateur directes » = [toward direct brain-computer communication](https://www.annualreviews.org/docserver/fulltext/biophys/2/1/annurev.bb.02.060173.001105.pdf?expires=1734631204&id=id&accname=guest&checksum=E59F1F2C70924CA6A83DCA4E804F0586), et invente pour la première fois le terme “Brain Computer Interface”. Ces travaux pionniers démontrent que les signaux cérébraux mesurés par EEG peuvent être utilisés pour contrôler un curseur sur un écran. Années 1970 : Premières expériences sur des animaux, notamment des singes, pour démontrer la possibilité de contrôler des neurones via des récompenses et punitions.

**1990-2000**
1998 : Philip Kennedy (neurologiste) a fait un travail important dans les BCI. Il réalise la première implantation invasive d'une ICO sur un patient humain paralysé. C'était un des premiers à faire une BCI invasive sur un humain. Le premier patient considéré comme le tout premier a avoir une BCI, Johnny Ray, était capable de contrôler un curseur. Ce patient, surnommé "le premier cyborg", y parvient après plusieurs mois d'entraînement. Implentera d'autres patients ensuite. Mais Kennedy manque de financements par la suite pour ses recherches. Alors il a subi une intervention chirurgicale pour se faire impleter lui-même en 2014. Il s'est enregistré pendant 80 jours avant d'enlever l'implant. [The father of cyborgs] (https://www.youtube.com/watch?v=_tm5ZaRCORg) (2021), documentaire sur lui (si tu veux le voir mets ton VPN aux USA et vas sur tubi), "my short-term goal is to help people who are locked in to communicate", "i think it's part of the evolution of the human race". "je posais les bases pour la prochaine génération. Pour réellement améliorer les humains normaux. Et la raison pour laquelle je pense ca, est que, vous savez, si vous connaissez Ray Kurzweil qui a prédit que l'IA serait au moins égale et surpasse problablement l'intelligence humaine en 2045, soit dans une génération". Rappelle la vision de Musk, on en parlera plus bas.

**2000-2010**
2003 : Miguel Nicolelis a fait un travail vraiment fascinant avec les BCI. [Monkey controls robot arm](https://www.youtube.com/watch?v=wxIgdOlT2cY). 2004 : L'entreprise Cyberkinetics développe le système BrainGate, permettant à des patients paralysés de contrôler des dispositifs externes comme des bras robotisés. 2006 : Des chercheurs réussissent à connecter une ICO à un bras robotique, permettant à un patient de saisir des objets par la pensée.

**2010-2020**
2012 : braingate, [femme controlant un bras robot](https://www.youtube.com/watch?v=QRt8QCx3BCo), serre une bouteille d'eau; et boit dedans. 2019 : Synchron devient la première entreprise à obtenir l'autorisation de la FDA pour tester une ICO invasive sur des humains aux États-Unis. 2023: commencent à créer des trucs comme les interfaces cerveau colonne vertébrale. Y a aussi Neuralink, Synchron, développement de flexible sensors, blackrock neurotech est aussi sur le sujet des flexible sensors, Meta et CTRL-Labs. Word decoding, image reconstruction... Beaucoup d'avancées

# II - Principes techniques

### 1. Cerveau, moelle épinière, nerfs

#### a. connaissances de base
Pour comprendre comment ça marche on va prendre l’exemple de tout le processus qui se déroule *quand une mouche se pose sur mon bras et que je bouge mon bras pour quelle parte*. Tout ce qui va être décrit ici est très simplifié dans le but que ce soit compréhensif. Mais avant d'expliquer tout le processus de la mouche, il y a trois éléments importants : le cerveau, la moelle épinière, et les nerfs.

![explicationsimage](https://github.com/user-attachments/assets/43e2d37d-d092-409c-877a-b7ee27b8efdf)

**Le cerveau** : on sait tous que dans le cerveau il y a des neurones (ou cellules nerveuses). Une cellule nerveuse ça a un corps cellulaire avec un noyau cellulaire, un axone, et des dendrites (A). Elles sont en très grande quantité, et les corps cellulaires de ces cellules nerveuses se trouvent essentiellement dans la partie grise du cerveau (B). Ca s'appelle le cortex, et c’est là que se passe le langage le mouvement etc (oui oui y a la majeure partie des trucs complexes se passent dans cette petite couche d'environ 3mm d'épaisseur). Les cellules communiquent entre elles grâce eux fibres nerveuses (c'est les axones des neurones) en utilisant des impulsions electriques, fibres qui sont principalement blanches et forment la matière blanche comme on peut l'observer (B). Donc en gros la partie blanche c'est surtout les cables quoi.

Ensuite, dans la continuité du cerveau : **la moelle épinière** (C). Concrètement c'est un cordon nerveux situé dans le canal vertébral, protégé par la colonne vertébrale. Elle s'étend du tronc cérébral jusqu'à la deuxième vertèbre lombaire. Sa structure interne comprend elle aussi, une substance grise centrale (idem, les corps cellulaires des neurones) et une substance blanche périphérique (là aussi, les axones cad les fibres nerveuses). 

Et enfin, dans la continuité de la colonne cérébrale et qui s'étend de partout jusqu'aux muscles : **les nerfs**. Les nerfs en fait, c'est des axones de neurones (D). Et tous ces nerfs qui partent de la colonne vertebralle c'est le système nerveux périphérique (contrairement au central qui est composé du cerveau + moelle épinière)

#### b. une mouche se pose sur mon bras
Allez c'est parti pour le processus de la mouche. *Une mouche se pose sur mon bras*.

<div align="center">
  <img src="https://github.com/user-attachments/assets/b5249036-757c-42f2-9f02-8c1d2d8d25df" alt="moucheai2">
  <p>image générée avec Flux 1.1 sur Mammouth.ai</p>
</div>

- **ETAPE 1 : je sens la mouche** : 
À la surface de la peau et dans d'autres tissus, il y a différents types de récepteurs sensitifs, qui détectent des stimulis physiques tels que la pression, la température et la douleur (E). Une fois qu'un récepteur est activé par un stimulus (comme notre mouche qui le touche), il génère un signal électrique. 
- **ETAPE 2 : signal transmis à la moelle épinière**
Ce signal électrique est transmis le long des axones des neurones sensitifs (= le long des fibres nerveuses) jusqu’au corps cellulaire du neurone qui se trouve dans les ganglions situés vers la moelle épinière (F). Les axones des neurones sensitifs rejoignent la moelle épinière par la racine dorsale (F). À ce niveau, ils établissent des connexions avec d'autres neurones dans la corne dorsale de la moelle épinière, permettant ainsi la transmission rapide de l'information sensorielle vers le cerveau. Beaucoup de termes techniques pour dire en gros l'info va dans la moelle épinière
- **ETAPE 3 : signal traité en partie par la moelle épinière et envoyé au cerveau**
Avant d'atteindre le cerveau, certaines informations sensorielles peuvent être traitées au niveau de la moelle épinière. C'est particulièrement vrai pour les réflexes, où une réponse rapide est nécessaire pour éviter une blessure (par exemple, retirer rapidement une main d'une surface chaude) (G).
Bref les neurones de la moelle épinière transmettent le signal au cerveau. 
- **ETAPE 4 : traitement par le cortex somatosensoriel (on se rappelle, le cortex = partie grise)**
Une fois arrivés au cerveau, ils atteignent principalement le cortex somatosensoriel (en particulier l'aire S1) qui est responsable du traitement des informations tactiles. C’est donc là que le signal est traité. C’est qu’à ce moment-là que je prends conscience de la présence de la mouche. Comme quand on se cogne parfois y a un temps de latence avant de sentir la douleur. Le cortex somatosensoriel est organisé de manière somatotopique, ce qui signifie que chaque partie du corps est représentée dans une zone spécifique du cortex (H). Ca permet au cerveau de localiser précisément où se trouve la mouche sur mon bras. Il évalue la situation et décide d'agir pour l'enlever.
- **ETAPE 5 : cortex moteur s’active et envoie des signaux au neurones moteurs**
Le cortex moteur, région du cerveau responsable du contrôle des mouvements, s'active pour planifier une réponse motrice appropriée. Le cortex moteur envoie des signaux aux neurones moteurs supérieurs, qui se trouvent dans le cortex et le tronc cérébral.
- **ETAPE 6 : transmission des signaux jusqu’aux muscles**
Ces signaux descendent ensuite dans la moelle épinière et activent les neurones moteurs inférieurs, qui transmettent les signaux aux muscles
- **ETAPE 7 : exécution du mouvement**
Les neurones moteurs stimulent les muscles du bras pour provoquer une contraction, permettant ainsi au bras de bouger et d'enlever la mouche.
En résumé, les neurones moteurs envoient un signal qui active les muscles via un neurotransmetteur, entraînant leur contraction et permettant le mouvement.

![explicationspart2](https://github.com/user-attachments/assets/546f2014-de09-4dfc-90bc-dd6899d54038)

Donc en gros y a une communication bidirectionnelle entre le cerveau et le reste du corps en passant par la moelle epinière et les nerfs


### 2. Fonctionnement global BCIs

Une BCI ou brain-computer interface est un système qui permet une communication directe entre le cerveau et un appareil externe, comme un ordinateur. Elle peut se composer de divers logiciels et outils différents selon les méthodes employées et le but dans lequel on l'utilise. Mais voilà un schéma global de son fonctionnement en 3 parties : 
- **La sonde, qui mesure l'activité cérébrale** :
C’est la partie en contact avec le tissu cérébral. L'activité électrique du cerveau est mesurée.
On distingue deux types de BCI : les invasives et les non invasives. Et il faut savoir que plus on est éloigné du cerveau, plus le signal est bruyant. On peut parfois obtenir la résolution d'activité d'un seul neurone. Nombreuses techniques pour récupérer le signal : EGoG, EEG (non invasive), MEA (very used), stereo EEG electrode sEEG...
- **Les électrodes sont reliées à un boîtier d'acquisition** :
C’est l'électronique qui connecte la sonde au monde extérieur. Ce boîtier convertit les signaux électriques captés par les électrodes en données numériques (en chiffres quoi). Avec ces étapes : 
d’abord il amplifie les signaux électriques très faibles (de l'ordre du microvolt) captés par les électrodes
ensuite il convertit ces signaux analogiques amplifiés en données numériques que l'ordinateur peut traiter
puis il envoie ces données numériques à l'ordinateur ou au dispositif de traitement pour l'analyse ultérieure qui dispose maintenant d’informations exploitables par les algorithmes de traitement et de classification
- **Le software interprète les données** :
varie selon les objectifs et les signaux. Mais cette partie peut consister en l’extraction des caractéristiques pertinentes (peuvent inclure la puissance dans différentes bandes de fréquences ou autres paramètres spécifiques). 
Analyse spectrale : Les séries de Fourier, notamment la transformée de Fourier rapide (FFT), sont utilisées pour décomposer les signaux EEG complexes en leurs composantes fréquentielles. Cela permet d'identifier les différents rythmes cérébraux présents dans le signal. 
Filtrage : Des techniques basées sur Fourier sont employées pour filtrer les signaux EEG, éliminant le bruit et isolant les bandes de fréquences d'intérêt, comme les ondes alpha ou bêta
Des algorithmes de classification, tels que les réseaux de neurones, les machines à vecteurs supports ou les analyses linéaires discriminantes, sont utilisés pour identifier l'état mental de l'utilisateur à partir des caractéristiques extraites


# III - Articles les plus intéressants / principales avancées
Il existe de nombreuses études et articles sur plein d'applications différentes. Mais j'ai décidé de me concentrer sur ce qui concerne la reconstruction du langage et des images + le mouvement. J'ai selectionné ici les études qui m'ont marqué.

### 1. Langage reconstruction
Dans la reconstruction du langage, deux études publiées en août 2023 montrent la création de BCIs capables de traduire des signaux neuronaux en phrases à une vitesse proche de celle d'une conversation normale (environ 150 mots par minute) :
|Article| [A high-performance neuroprosthesis for speech decoding and avatar control](https://www.nature.com/articles/s41586-023-06443-4) |[A high-performance speech neuroprosthesis](https://www.nature.com/articles/s41586-023-06377-x)|
|----------|-------|-------|
|Présentation|étude du Chang Lab (le laboratoire soutenu par Meta qui essaye de comprendre comment le cerveau traite le langage on en parle en dessous) L’article s’appuie sur leurs travaux précédents, qui ont montré qu’il est possible d'enregistrer l’activité neuronale d’une personne paralysée qui tente de parler et de traduire cette activité en mots et en phrases sous forme de texte sur un écran. Ils ont avec ces études pour projet de développer un appareil capable d’aider les personnes qui ont perdu la capacité de parler à communiquer à nouveau.  |C’est un projet de recherche collaboratif Braingate = équipe de recherche, basés dans divers instituts aux États-Unis, ont créé une interface cerveau-ordinateur de conversion de la parole en texte |
|Technologies utilisées| 1. réseau d'électrodes d'électrocorticographie ECoG a été utilisé pour collecter des signaux placé sur les zones liées à la parole du cortex sensorimoteur et du gyrus temporal supérieur. 2. RNN (réseau neuronal récurrent) = associe les caractéristiques de l’ECoG aux mouvements des articulateurs qui font partie du conduit vocal. Puis traduit en phrases avec : 3. modèle de langage = aider à générer les phrases les plus plausibles ==> neural signal processing, speech detection, word classification, language modeling|1. quatre réseaux de microélectrodes intracorticales implantées pour collecter les signaux 2. réseau neuronal récurrent : ils ont entrainé un RNN pour décoder l’activité neuronale en phonème séquences probabilities. 3. modèle de langage pour les décoder en mots : ces séquences ont ensuite été assemblées into the most likely sequence of words being spoken by an n-gram language model.  |
|Process de l’étude|Avec un participant souffrant d’une paralysie grave des membres et de la voix, causée par un accident vasculaire cérébral du tronc cérébral. La patiente active la BCI simplement en tentant de parler, elle essaye de dire des phrases. Mais vu qu’elle est paralysée elle le dit pas. Et en gros, leur appareil lit le plan (the blueprint) des instructions que le cerveau utilise pour transmettre aux muscles du conduit vocal.  | Avec un patient atteint de ALS|
|Résultats| ils ont réussi à traduire les tentatives de discours à un rythme d’environ 78 mots par minute. Un taux d'erreur de 25,5 % pour un vocabulaire de 1 024 mots. L’approche a permis d’obtenir un décodage cerveau-texte avec un taux médian de 78 mots par minute et un taux médian d’erreur de mots de 25 % pour un vocabulaire de 1 024 mots. Sachant que le vocabulaire pratique et quotidien d'un individu peut varier de 300 à 3 000 mots. Pour un adulte moyen, on estime qu'il utilise environ 500 mots différents par jour dans la vie courante. | 1. Avec une contrainte de 50 mots de vocabulaire, leur BCI a détecté quasi tous les mots correctement, le premier jour d’utilisation. taux d’erreur de 9,1 % sur un vocabulaire de 50 mots 2. Avec un vocabulaire de 125 000 mots, ce qui est vraiment très volumineux, ils ont décodé des phrases avec un taux d'erreur de moins de 10% le deuxième jour d’utilisation. taux d’erreur de 23,8 %. 3. La tentative de parole de notre participant a été décodée à 62 mots par minute|
|Différences|électrodes placées sur la surface du cortex cérébral, souvent sous la dure-mère, et mesurent l'activité électrique à partir de cette position. Elles sont moins invasives que les microélectrodes intracorticales. Différence dans les modèles IA utilisés |microélectrodes intracorticales sont implantées directement dans le tissu cérébral. Elles permettent d'enregistrer l'activité neuronale à un niveau très localisé : l'activité de neurones individuels ou de petits groupes de neurones. + de précision mais + invasif|

### 2. Image reconstruction

#### a. Mindeye

[Article explicatif](https://stability.ai/research/minds-eye)
[Github](https://github.com/MedARC-AI/fMRI-reconstruction-NSD)
MindEye, développé par StabilityAI (qui a créé le modèle IA StableDiffusion, d'ailleurs disponible sur Mammouth.ai), est une approche novatrice qui permet de reconstruire et de récupérer des images à partir de l'activité cérébrale mesurée par IRMf (projet open source). A partir d'un échantillon d'activité IRMf d'un participant visualisant une image, MindEye peut soit identifier quelle image parmi un ensemble d'images candidates possibles était l'image vue d'origine (récupération), soit recréer l'image qui a été vue (reconstruction). 

**Technologies utilisées** : 
- Utilise l'IRMf pour mesurer l'activité cérébrale
- approche consistant à utiliser deux sous-modules parallèles spécialisés pour la récupération (en utilisant l'apprentissage contrastif) et la reconstruction (en utilisant une diffusion préalable). 
**Process de l'étude** : Chaque image unique de l'ensemble de données a été visualisée trois fois, pendant trois secondes à la fois. L'activité IRMf correspondante (motif spatial aplati sur des cubes de tissu cortical de 1,8 mm appelés « voxels ») a été collectée pour chaque présentation d'image. L'activité IRMf sur les trois visualisations de la même image a été moyennée et saisie dans MindEye pour récupérer et reconstruire l'image d'origine. 
**Résultats** : Peut retrouver l'image exacte correspondante avec une précision de 93,2% parmi un ensemble d'échantillons de test. Bonnes reconstructions aussi.

Resultats reconstruction :
![mindeye](https://github.com/user-attachments/assets/7f525b7b-63b3-40a1-a3c9-c208d59a0910)


#### b. Toward a real-time decoding of images from brain activity, Meta AI FAIR Research

[Article qui résume](https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=research&utm_content=video)
[Article de recherche](https://arxiv.org/abs/2310.19812)

Octobre 2023, recherche d'une équipe de chercheurs du FAIR, but de développer un système IA capable de décoder le déroulement des représentations visuelles dans le cerveau. Ce système d’IA peut être déployé en temps réel pour reconstituer, à partir de l’activité cérébrale, les images perçues et traitées par le cerveau à chaque instant. Cela ouvre une voie importante pour aider la communauté scientifique à comprendre comment les images sont représentées dans le cerveau, puis utilisées comme fondements de l’intelligence humaine.

<div align="center">
  <img src="https://github.com/user-attachments/assets/ce35ebc3-62b6-4f56-a407-db31b6734b32" alt="resultsmeta">
  <p>(A) générations obtenues dans le temps 0 à 1,5s (B) full window generations</p>
</div>

**Technologies utilisées** : 
- utilisant la magnétoencéphalographie (MEG), non invasif
- ont exploité l'architecture récentre formée dans l'étude de août 2022 [Decoding speech perception from non-invasive brain recordings](https://ai.meta.com/blog/ai-speech-brain-activity/)
- développent un système en trois parties composé d'un encodeur d'image, d'un encodeur cérébral et d'un décodeur d'image. L'encodeur d'image construit un riche ensemble de représentations de l'image indépendamment du cerveau. L'encodeur cérébral apprend ensuite à aligner les signaux MEG sur ces intégrations d'images.Meilleure alignement des signaux cérébraux avec les systèmes de vision par ordinateur modernes comme DINOv2. Enfin, le décodeur d'image génère une image plausible à partir de ces représentations cérébrales.

**Process de l'étude** :
Des gens voyaient des images dans un scanner de neuroimagerie. Et le but est de créer un système IA qui reconstruit ce qu’ils voient seulement a partir de l’activité cérébrale.  Les volontaires ont vu les images 1,5 secondes, et on peut observer les resultats dans le temps (A).

**Résultats** : Bien que les images générées restent imparfaites, les résultats suggèrent que l'image reconstruite préserve un riche ensemble de caractéristiques de haut niveau, telles que les catégories d'objets.
Cependant, le système d'IA génère souvent des caractéristiques de bas niveau inexactes en plaçant ou en orientant mal certains objets dans les images générées.

### 3. Mouvement
Synchron et utilisation du vision pro

# IV - Le cerveau, infinie complexité

Le cortex : C’est l'endroit où se déroule la plupart des actions : c'est grâce à lui qu’on peut  penser, bouger, sentir, voir, entendre, se souvenir, parler et comprendre le langage. Mais en réalité, c’est que les 1 à 4 millimètres extérieurs du cerveau, la couche externe quoi. Et la majeure partie de l’espace en dessous, c’est cette substance blanche, en gros… des câbles. Et cette couche, elle s'étend sur environ 2000 centimètres carrés. Comment c’est possible ?  Grâce à tous les replis. Donc si on retire le cortex du cerveau, qu’on l’aplati, on se retrouve avec ça. Une feuille d’environ 3 mm d'épaisseur et une surface de 2 000 à 2 400 cm2 , soit environ la taille d'un carré de 48 cm x 48 cm. Une serviette de table. Et à côté, voilà la vraie taille d’un cerveau.

Mise à l’échelle : Maintenant pour se rendre compte de ce qu’il se passe à l’intérieur multiplions le cerveau par 1 000.
* 1000 ça veut dire que 1mm = 1m. Donc chaque millimètre du cerveau fait désormais un mètre.
- 48cm = 480m : la serviette en cortex, qui mesurait environ 48 cm de chaque côté, a maintenant un côté de 480m. L’équivalent de notre dame à là. Il faudrait environ 24 minutes là pour faire le tour du tracé à 5 km/h.(480*4 = 1920m)
- 16cm = 160m : et le cerveau dans son ensemble s’intégrerait désormais parfaitement dans un carré de la longueur de côté de Notre Dame. 
- son épaisseur de 3 mm correspond désormais à 3 mètres

Observons ce qu’il y a dans un cube : Maintenant que le cerveau à une échelle représentative, on peut maintenant aller dans la rue, et observer facilement ce qui se passe à l’intérieur de ces deux mètres d’épaisseur. Sortons un cube d’1m sur 1m et examinons le.

- Les somas : c’est ça, la partie la plus volumineuse du neurone. Quelle taille ça fait ?
En gros ça varie en taille, mais apparemment les somas des neurones du cortex ont souvent un diamètre d’environ 10 ou 15 micromètre. On va prendre 10, donc 10 micromètres × 1000 = 10 000 micromètres, soit 1 cm. Donc les somas ils font entre 1 et 1,5cm. La taille d’une pièce bille.
Et combien y a t il de somas dans 1 mm cube de cortex ?
Il y en a environ 20 milliards dans le cortex entier, qui fait 500 000 millimètres cubes. Donc 20 milliards divisé par 500k = 40 000 somas pour 1mm cube. Donc dans notre cube qui représente 1mm cube, on en a 40 000. Ca fait une bille tous les carrés de 3cm*3cm

Mais le soma c’est qu’une petite partie du neurone. 
- A chaque soma y a des dendrites qui partent (jusqu'à des centaines de dendrite par neurone). Qui peuvent s’étirer sur plusieurs centaines de micro-mètres, jusqu’à 1mm. Soit jusqu'à 1 m, dans de nombreuses directions différentes.
- A l’autre extrémité, il y a un axone qui peut mesurer jusqu’à 1m de long. Et chacun de ces axones est d’une épaisseur d’environ un millimètre seulement, ce qui donne un enchevêtrement de spaghettis électriques. Chaque neurone a des connexions synaptiques avec pas moins de 1 000, parfois même 10 000 autres neurones. Et il y a 20 milliards de neurones au total dans le cortex * 1000 connexions = il y a plus de 20 000 milliards de connexions neuronales individuelles dans le cortex. Dans notre seul mètre cube, il y a plus de 20 millions de synapses.

- les câbles qui passent par le cube : Mais c’est pas tout, parce que non seulement il y a des spaghettis qui sortent de chacune des 40 000 billes de notre cube, mais des milliers d'autres spaghettis traversent notre cube en provenance d'autres parties du cortex.
- cellules gliales : Il s'avère qu'il existe d'autres cellules dans le cerveau appelées cellules gliales, qui nourrissent, protègent les neurones, éliminent les déchets, etc. Et il y en a à peu près le même nombre qu'il y a de neurones. Soit environ 40 000 dans notre cube.
- vaisseaux sanguins : Enfin, les vaisseaux sanguins. Dans chaque millimètre cube du cortex, il y a 230 millimètres de vaisseaux sanguins.*1000 =  230 mètres dans notre cube.

Conclusion : Rappelons maintenant qu'en réalité, tout ce qui se trouve dans notre boîte tient dans un millimètre cube. Et ce mm cube, avec tout ce qui se passe à l’intérieur, ça représente qu'un 500 000e du cortex. Si on décomposait tout le cortex géant en cubes d'un mètre de côté et qu’on les alignait, ils s'étendraient sur 500 km. Paris-Bordeaux. Et à tout moment où on s’arrêterait sur la route… on pourrait regarder le cube devant lequel on est et il contiendrait toute cette complexité à l'intérieur. Et tout ça, c’est en ce moment dans notre cerveau.

# V - La course à l'alumette


### 1. Meta & BCIs
**Investissements de Meta dans les BCI** :
- entre 500 millions et 1 milliard de dollars pour acquérir CTRL-Labs selon Bloomberg
- une des plus importantes acquisitions ou la plus importante réalisée par Méta depuis le rachat d'Oculus en 2014
- investissements dans le Chang Lab et dans la technique infra-rouge

**Annonce du projet** : Tout a commencé par l'annonce au F8 (événement annuel organisé par meta depuis 2007, présentent les nouvelles fonctionnalités, outils et mise à jours pour les développeurs et partager des idées sur l'avenir des technologies) 2017, où Facebook, Regina Dugan annonce chercher à développer un appareil BCI portable et non invasif pour la parole. 3:16 "nous n'avons exploré que la surface de ce qui est possible. Il y a beaucoup, beaucoup plus à faire". 5:44 "so... what if you could type directly from your brain ? It sounds impossible mais c'est plus proche que ce que vous ne pensez. Ils lancent alors le programme de recherche sur les BCIs au Reality Labs. A brain mouse

**Création du silent speech interface project, au reality labs** :
Meta ouvre alors un programme de recherche sur l'interface cerveau ordinateur au Reality Labs, la division dédiée au développement de technologies de réalité virtuelle ou réalité augmentée. Son but : développer une interface vocale silencieuse et non invasive qui permettra aux utilisateurs de taper simplement en imaginant les mots qu’ils veulent dire. C’est le silent speech interface project.

**Chang Lab & Reality Lab** : 
Le but de Meta était de créer un appareil portable non invasif qui permet aux gens de taper simplement en s'imaginant parler. Et pour ça, selon Mark Chevillet, directeur du BCI research program, il fallait déjà déterminer si une interface de conversion de la pensée en parole était possible. 
Pour cela, Meta a décidé de parrainer et soutenir financièrement une équipe de chercheurs de l’UCSF (University of California, San Francisco, institution de premier plan dans la recherche biomédicale aux États-Unis) : le Chang Lab. Le but de leurs recherches c’est de développer une prothèse pour restaurer la communication vocale chez les personnes qui ont perdu la capacité de parler. Et ça, en détectant la parole intentionnelle à partir de l'activité cérébrale en temps réel. MAIS avec une technique qui consiste à placer des électrodes à la surface du cerveau. Alors que les électrodes implantées lisent les données de neurones individuels, cette technique, appelée électrocorticographie mesure simultanément des groupes assez importants de neurones. 
Chevillet explique que Meta espérait qu'il serait également possible de détecter des signaux équivalents provenant de l'extérieur de la tête. Leurs recherches "aident à éclairer la voie à suivre dans notre mission de développement d'une interface vocale silencieuse non invasive pour la prochaine plate-forme informatique". C’est-à-dire à "parvenir à une BCI entièrement non invasive comme solution d'entrée potentielle pour les lunettes de AR. 
développent aussi technique infrarouge au Mallinckrodt Institute of Radiology de la Washington University School of Medicine et l'APL de Johns Hopkins

**CTRL Labs** : En parallèle, toujours au Reality Labs, Facebook achète CTRL Labs en septembre 2019. La start-up new-yorkaise a créé un bracelet capable de transformer des impulsions envoyées par le cerveau à la main en commande informatique. Un bracelet pour anticiper les mouvements physiques. 
Et difficile d’avoir de bons signaux en non invasif, or Meta veut absolument du non-invasif. Et ils voient court terme ⇒ se sont finalement rétractés sur le bracelet

**Arrêt du projet Silent Speech + se concentrent sur le bracelet** : Meta a arrêté car ils ont compris que c’était pas tout de suite pour la commercialisation. Donc se sont tournés vers le bracelet. Mais continuent les recherches au FAIR. ont conclu : est-il possible de décoder la parole à partir de l’activité cérébrale, et est-il possible de la décoder avec un appareil optique portable ? », explique Chevillet. « Nous pensons que cela sera finalement possible.»

**Interface Neurale généralisable basée sur un bracelet** : 
En 2022 ils sortent [cette vidéo](https://www.youtube.com/watch?v=Kx_nVrEKwTE ) où ils montrent ce qu’ils souhaitent arriver à faire avec le bracelet.
Applications : 
tapper sur un clavier sans avoir besoin de trop bouger
interaction avec des interfaces de réalité augmentée
jouer à des jeux
Et le 28 février 2024, ils publient cet article : A generic noninvasive neuromotor interface for human-computer interaction. Où ils présentent où ils en sont dans leurs recherches. Ils présentent leur interface neuro-motrice non invasive qui utilise une technique qui permet d’étudier l’activité électrique des muscles qui s’appelle l'électromyographie de surface (sEMG). Et qui permet diverses formes d'interaction avec des machines. 
la navigation, sélectionner des éléments
la reconnaissance de gestes légers comme taper avec les doigts ou slider
Entrer du texte en écrivant avec la main [video](https://www.youtube.com/watch?v=GAagRSQSUaU)

**C’est pour bientôt** : interview avec Morning Brew Daily, datant de février dernier. "and it's only going to get crazier." "Aujourd'hui tu tappes sur un ordinateur ou un téléphone. Tu parles à tes lunettes. Mais aujourd'hui une des choses les plus folles sur lesquelles nous travaillons et cette interface neuronale. "La façon dont votre cerveau communique avec votre corps et contrôle la façon dont vous bougez en envoyant des signaux au système nerveux à vos différents muscles". Et vous pouvez les récupérer avec un bracelet EMG.
"You'll be able to, in the future, essentially just like type and control something by kind of thinking about how you want to move your hand," but like it won't even be like big motions. I can just sit here and basically typing something to an AI. You will have this complete private and discreet interface où tu peux tout au long de la journée text tes amis ou ton ai et avoir la réponse en temps réel. "I think that just gonna be insane". Ça fait plusieurs années qu'ils bossent dessus, et ils sont proches "nous sommes en fait assez proches d'avoir ici un produit dans les prochaines années" "i'm pretty excited about that".

**Temporalité** : Vision court terme + non invasif

### 2. Apple & BCIs
### 3. Google & BCIs
### 4. Bill Gates, Jeff Bezos & Synchron
### 5. Neuralink & Elon vision
Neuralink est un périphérique d’entrée-sortie généralisé. Il lit les signaux électriques et génère des signaux électriques. Et tout ce que t’as expérimenté dans ta vie : l'odeur, les émotions, ce sont tous des signaux électriques. C’est un peu bizarre de penser que toute ton expérience de vie est distillée en signaux électriques pour les neurones. Mais c’est effectivement le cas. Du moins c’est ce que toutes les preuves indiquent. Donc si vous déclenchez le bon neurone, vous pourriez déclencher une odeur particulière, vous pourriez certainement faire briller les choses. Faire à peu près n'importe quoi.

**Investissements dans Neuralink**
- au moins 643 millions de dollars les sommes levées depuis le début
- juillet 2024, Neuralink a discrètement levé 43 millions de dollars supplémentaires
- août et novembre 2023, Neuralink a levé environ 323 millions de dollars auprès d'investisseurs
- juillet 2021, l'entreprise avait bouclé une levée de fonds de 195 millions d'euros
- avait déjà obtenu un financement de 152 milliards d'euros par l'un des anciens cofondateurs de l'entreprise

**Vision long terme** : Mais l’aspiration à long terme de Neuralink est d’améliorer la symbiose entre humain et IA. Et ça, en augmentant la bande passante de la communication entre les deux. Elon Musk a souvent parlé de menaces, de problèmes de sécurité de l’IA. Et lorsqu’on lui demande s’il pense que Neuralink est la meilleure approche actuelle dont on dispose pour la sécurité de l’IA. Il répond que c’est une idée qui pourrait contribuer à la sécurité de l’IA, même si ce n'est pas la solution universelle ou quelque chose de sûr. Il explique qu’il y a plusieurs années il s’est demandé “qu’est ce qui empêcherait l’alignement de la volonté humaine collective avec l’IA ? Et sa réponse était le faible débit de données des humains. En particulier notre faible taux de production. 

**Problème qui empêcherait l’alignement humain / AI** : Il dit que l’être humain communique doucement. Pour communiquer il faut au moins dans une certaine mesure, modéliser l’état d’esprit de la personne à qui vous parlez, alors prenez le concept que vous essayez de transmettre, compressez cela en un petit nombre de syllabes, dites les et espérez que l’autre personne les décompresse en une structure conceptuelle qui est aussi proche que possible de ce que vous avez en tête. Quand t’as des concepts complexes t’es obligé de les distiller en ce qui est le plus essentiel. Dans le principe de compression vous résumez les choses à ce qui compte le plus. Parce que vous ne pouvez dire que peu de choses. Je pense que si notre débit augmente, on deviendrait beaucoup plus verbaux. Il évalue ce débit en bits par seconde, BPS. La moyenne de bits par seconde d’un humain est de moins de 1 bit par seconde au cours d’une journée, 86400 secondes dans une journée, et vous ne communiquez pas à 86400 tokens par jour, par conséquent vos bits par seconde sont inférieurs à un. Sauf que l’IA communique en terabits par seconde alors que tu communiques en bits par seconde. Le téléphone et l'ordinateur essayent juste de comprendre ce que tu veux, et c’est extrêmement long (32:50). Il y a une éternité entre chaque frappe d’un point de vue informatique. Si vous avez des ordinateurs qui peuvent exécuter des milliards d’instructions par seconde, et une seconde entière s’est écoulée, ca fait un milliard de choses qu’il aurait pu faire pendant cette seconde.

**Solution** : Il pense qu’on pourrait mieux aligner la volonté humaine collective sur l’IA si le taux de sortie était considérablement augmenté. Et justement, pour lui, il y a un potentiel d’augmentation en sortie.
Lex : Donc plus on augmente ça signifie que plus le débit de données que les humains peuvent absorber et produire, plus on a des chances dans un monde plein d’AGI ?
Elon : Yeah. Lex : le taux de sortie serait augmenté en augmentant le nombre d'électrodes, le nombre de canaux ? et peut-être en implantant plusieurs Neuralinks ?
Elon : Yeah

**Résultats** : Musk dit qu’avec son premier patient, ils ont réussi à atteindre 1 bit par seconde. Il dit que leur objectif est d’avoir une centaine de bits par seconde, et peut-être dans 5 ans un mégabit par seconde. Ce serait plus rapide que la vitesse à laquelle n’importe quel humain communique en tapant ou en parlant. A partir d’un certain nombre de bits par seconde, une toute nouvelle manière de communiquer avec les ordinateurs et entre humains serait débloquée. D’où la raison pour laquelle son implant s’appelle telepathy. Parler rapidement direct en pensée avec les machines et voire d’autres humains.

**Stratégie pour arriver à l’augmentation** : Mais les premières années de l’entreprise sont là pour résoudre des dommages neurologiques fondamentaux. C’est la chose la plus censée à faire selon Elon. Commencer par résoudre les problèmes fondamentaux des lésions neuronales de base. (17:05) Pourquoi commencer par le médical ? Pour le risque. En effet, il n’y a pas de risque 0 avec des nouveaux appareils. On ne peut pas réduire le risque à 0. Or, si quelqu’un est capable d’avoir une profonde amélioration de sa communication, parce que de base il a des dommages neurologiques, alors le risque en vaut la peine. Et à mesure qu’ils arrivent à réduire le risque, et imaginons qu’il y ai 1000 personnes implantées qui l'utilisent depuis des années alors on peut commencer à envisager l’augmentation. Mais pour ça, ils souhaitent commencer l’augmentation avec des personnes ayant des lésions neuronales. Leur objectif n’est pas seulement celui de permettre aux gens de communiquer, d’avoir un débit de données équivalent à celui des humains normaux, non, ils veulent réussir à leur donner un débit de communication supérieur à celui des humains normaux.

**Temporalité** : “Est-ce que tu penses qu’il y aura un monde dans les deux prochaines décennies où des centaines de millions de personnes ont des Neuralinks ? Oui. Je le pense. (32:00). On est sur une entreprise qui pense à long terme. “Ce serait dans 10 ou 15 ans”.
Lex : quand est ce que je pourrai en avoir un ?
Elon : probablement moins de 10 ans.
Vision long terme + invasif






